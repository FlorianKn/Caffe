I1204 14:36:36.930032 17293 upgrade_proto.cpp:1084] Attempting to upgrade input file specified using deprecated 'solver_type' field (enum)': /home/nvidia/.local/install/caffe/models/Example/solver.prototxt
I1204 14:36:36.930640 17293 upgrade_proto.cpp:1091] Successfully upgraded file specified using deprecated 'solver_type' field (enum) to 'type' field (string).
W1204 14:36:36.930656 17293 upgrade_proto.cpp:1093] Note that future Caffe releases will only support 'type' field (string) for a solver's type.
I1204 14:36:36.930882 17293 upgrade_proto.cpp:1113] snapshot_prefix was a directory and is replaced to /home/nvidia/.local/install/caffe/models/Example/solver
I1204 14:36:36.930953 17293 caffe.cpp:204] Using GPUs 0
I1204 14:36:36.938102 17293 caffe.cpp:209] GPU 0: NVIDIA Tegra X2
I1204 14:36:37.666352 17293 solver.cpp:45] Initializing solver from parameters: 
test_iter: 1
test_interval: 500
base_lr: 0.001
display: 100
max_iter: 3000
lr_policy: "fixed"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
stepsize: 300
snapshot: 1000
snapshot_prefix: "/home/nvidia/.local/install/caffe/models/Example/solver"
solver_mode: GPU
device_id: 0
net: "models/Example/example.prototxt"
train_state {
  level: 0
  stage: ""
}
type: "Nesterov"
I1204 14:36:37.666906 17293 solver.cpp:102] Creating training net from net file: models/Example/example.prototxt
I1204 14:36:37.667796 17293 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer MyData
I1204 14:36:37.667879 17293 net.cpp:53] Initializing net from parameters: 
name: "example_Network"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "MyData"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "dataset/train.txt"
    batch_size: 64
    shuffle: true
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
      variance_norm: AVERAGE
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "dropout1"
  type: "Dropout"
  bottom: "pool1"
  top: "pool1"
  dropout_param {
    dropout_ratio: 0.1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 48
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
      variance_norm: AVERAGE
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "dropout2"
  type: "Dropout"
  bottom: "pool2"
  top: "pool2"
  dropout_param {
    dropout_ratio: 0.3
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      variance_norm: AVERAGE
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "dropout3"
  type: "Dropout"
  bottom: "conv3"
  top: "conv3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc5"
  type: "InnerProduct"
  bottom: "conv3"
  top: "fc5"
  param {
    lr_mult: 10
    decay_mult: 1
  }
  param {
    lr_mult: 20
    decay_mult: 0
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
      variance_norm: AVERAGE
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "drop4"
  type: "Dropout"
  bottom: "fc5"
  top: "fc5"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "fc5"
  top: "fc6"
  param {
    lr_mult: 10
    decay_mult: 1
  }
  param {
    lr_mult: 20
    decay_mult: 0
  }
  inner_product_param {
    num_output: 30
    weight_filler {
      type: "xavier"
      variance_norm: AVERAGE
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "EuclideanLoss"
  bottom: "fc6"
  bottom: "label"
  top: "loss"
}
I1204 14:36:37.668664 17293 layer_factory.hpp:77] Creating layer MyData
I1204 14:36:37.668707 17293 net.cpp:86] Creating Layer MyData
I1204 14:36:37.668730 17293 net.cpp:382] MyData -> data
I1204 14:36:37.668783 17293 net.cpp:382] MyData -> label
I1204 14:36:37.668812 17293 hdf5_data_layer.cpp:81] Loading list of HDF5 filenames from: dataset/train.txt
I1204 14:36:37.668903 17293 hdf5_data_layer.cpp:95] Number of HDF5 files: 1
I1204 14:36:37.671526 17293 hdf5.cpp:33] Datatype class: H5T_FLOAT
I1204 14:36:38.325181 17293 net.cpp:124] Setting up MyData
I1204 14:36:38.325242 17293 net.cpp:131] Top shape: 64 1 96 96 (589824)
I1204 14:36:38.325296 17293 net.cpp:131] Top shape: 64 30 (1920)
I1204 14:36:38.325314 17293 net.cpp:139] Memory required for data: 2366976
I1204 14:36:38.325335 17293 layer_factory.hpp:77] Creating layer conv1
I1204 14:36:38.325389 17293 net.cpp:86] Creating Layer conv1
I1204 14:36:38.325405 17293 net.cpp:408] conv1 <- data
I1204 14:36:38.325439 17293 net.cpp:382] conv1 -> conv1
I1204 14:36:38.327247 17293 net.cpp:124] Setting up conv1
I1204 14:36:38.327283 17293 net.cpp:131] Top shape: 64 20 92 92 (10833920)
I1204 14:36:38.327304 17293 net.cpp:139] Memory required for data: 45702656
I1204 14:36:38.327433 17293 layer_factory.hpp:77] Creating layer relu1
I1204 14:36:38.327481 17293 net.cpp:86] Creating Layer relu1
I1204 14:36:38.327494 17293 net.cpp:408] relu1 <- conv1
I1204 14:36:38.327512 17293 net.cpp:369] relu1 -> conv1 (in-place)
I1204 14:36:38.327533 17293 net.cpp:124] Setting up relu1
I1204 14:36:38.327543 17293 net.cpp:131] Top shape: 64 20 92 92 (10833920)
I1204 14:36:38.327558 17293 net.cpp:139] Memory required for data: 89038336
I1204 14:36:38.327569 17293 layer_factory.hpp:77] Creating layer pool1
I1204 14:36:38.327585 17293 net.cpp:86] Creating Layer pool1
I1204 14:36:38.327596 17293 net.cpp:408] pool1 <- conv1
I1204 14:36:38.327610 17293 net.cpp:382] pool1 -> pool1
I1204 14:36:38.327711 17293 net.cpp:124] Setting up pool1
I1204 14:36:38.327725 17293 net.cpp:131] Top shape: 64 20 46 46 (2708480)
I1204 14:36:38.327741 17293 net.cpp:139] Memory required for data: 99872256
I1204 14:36:38.327754 17293 layer_factory.hpp:77] Creating layer dropout1
I1204 14:36:38.327772 17293 net.cpp:86] Creating Layer dropout1
I1204 14:36:38.327788 17293 net.cpp:408] dropout1 <- pool1
I1204 14:36:38.327805 17293 net.cpp:369] dropout1 -> pool1 (in-place)
I1204 14:36:38.327863 17293 net.cpp:124] Setting up dropout1
I1204 14:36:38.327877 17293 net.cpp:131] Top shape: 64 20 46 46 (2708480)
I1204 14:36:38.327893 17293 net.cpp:139] Memory required for data: 110706176
I1204 14:36:38.327903 17293 layer_factory.hpp:77] Creating layer conv2
I1204 14:36:38.327934 17293 net.cpp:86] Creating Layer conv2
I1204 14:36:38.327945 17293 net.cpp:408] conv2 <- pool1
I1204 14:36:38.327963 17293 net.cpp:382] conv2 -> conv2
I1204 14:36:38.329166 17293 net.cpp:124] Setting up conv2
I1204 14:36:38.329192 17293 net.cpp:131] Top shape: 64 48 42 42 (5419008)
I1204 14:36:38.329210 17293 net.cpp:139] Memory required for data: 132382208
I1204 14:36:38.329236 17293 layer_factory.hpp:77] Creating layer relu2
I1204 14:36:38.329253 17293 net.cpp:86] Creating Layer relu2
I1204 14:36:38.329263 17293 net.cpp:408] relu2 <- conv2
I1204 14:36:38.329279 17293 net.cpp:369] relu2 -> conv2 (in-place)
I1204 14:36:38.329298 17293 net.cpp:124] Setting up relu2
I1204 14:36:38.329308 17293 net.cpp:131] Top shape: 64 48 42 42 (5419008)
I1204 14:36:38.329321 17293 net.cpp:139] Memory required for data: 154058240
I1204 14:36:38.329331 17293 layer_factory.hpp:77] Creating layer pool2
I1204 14:36:38.329408 17293 net.cpp:86] Creating Layer pool2
I1204 14:36:38.329419 17293 net.cpp:408] pool2 <- conv2
I1204 14:36:38.329435 17293 net.cpp:382] pool2 -> pool2
I1204 14:36:38.329532 17293 net.cpp:124] Setting up pool2
I1204 14:36:38.329550 17293 net.cpp:131] Top shape: 64 48 21 21 (1354752)
I1204 14:36:38.329566 17293 net.cpp:139] Memory required for data: 159477248
I1204 14:36:38.329599 17293 layer_factory.hpp:77] Creating layer dropout2
I1204 14:36:38.329620 17293 net.cpp:86] Creating Layer dropout2
I1204 14:36:38.329632 17293 net.cpp:408] dropout2 <- pool2
I1204 14:36:38.329646 17293 net.cpp:369] dropout2 -> pool2 (in-place)
I1204 14:36:38.329696 17293 net.cpp:124] Setting up dropout2
I1204 14:36:38.329710 17293 net.cpp:131] Top shape: 64 48 21 21 (1354752)
I1204 14:36:38.329725 17293 net.cpp:139] Memory required for data: 164896256
I1204 14:36:38.329738 17293 layer_factory.hpp:77] Creating layer conv3
I1204 14:36:38.329763 17293 net.cpp:86] Creating Layer conv3
I1204 14:36:38.329773 17293 net.cpp:408] conv3 <- pool2
I1204 14:36:38.329792 17293 net.cpp:382] conv3 -> conv3
I1204 14:36:38.330845 17293 net.cpp:124] Setting up conv3
I1204 14:36:38.330870 17293 net.cpp:131] Top shape: 64 64 19 19 (1478656)
I1204 14:36:38.330888 17293 net.cpp:139] Memory required for data: 170810880
I1204 14:36:38.330916 17293 layer_factory.hpp:77] Creating layer relu3
I1204 14:36:38.330934 17293 net.cpp:86] Creating Layer relu3
I1204 14:36:38.330945 17293 net.cpp:408] relu3 <- conv3
I1204 14:36:38.330960 17293 net.cpp:369] relu3 -> conv3 (in-place)
I1204 14:36:38.330976 17293 net.cpp:124] Setting up relu3
I1204 14:36:38.330986 17293 net.cpp:131] Top shape: 64 64 19 19 (1478656)
I1204 14:36:38.331001 17293 net.cpp:139] Memory required for data: 176725504
I1204 14:36:38.331010 17293 layer_factory.hpp:77] Creating layer dropout3
I1204 14:36:38.331025 17293 net.cpp:86] Creating Layer dropout3
I1204 14:36:38.331034 17293 net.cpp:408] dropout3 <- conv3
I1204 14:36:38.331048 17293 net.cpp:369] dropout3 -> conv3 (in-place)
I1204 14:36:38.331101 17293 net.cpp:124] Setting up dropout3
I1204 14:36:38.331113 17293 net.cpp:131] Top shape: 64 64 19 19 (1478656)
I1204 14:36:38.331128 17293 net.cpp:139] Memory required for data: 182640128
I1204 14:36:38.331140 17293 layer_factory.hpp:77] Creating layer fc5
I1204 14:36:38.331158 17293 net.cpp:86] Creating Layer fc5
I1204 14:36:38.331172 17293 net.cpp:408] fc5 <- conv3
I1204 14:36:38.331190 17293 net.cpp:382] fc5 -> fc5
I1204 14:36:38.515139 17293 net.cpp:124] Setting up fc5
I1204 14:36:38.515192 17293 net.cpp:131] Top shape: 64 500 (32000)
I1204 14:36:38.515223 17293 net.cpp:139] Memory required for data: 182768128
I1204 14:36:38.515254 17293 layer_factory.hpp:77] Creating layer drop4
I1204 14:36:38.515280 17293 net.cpp:86] Creating Layer drop4
I1204 14:36:38.515291 17293 net.cpp:408] drop4 <- fc5
I1204 14:36:38.515311 17293 net.cpp:369] drop4 -> fc5 (in-place)
I1204 14:36:38.515388 17293 net.cpp:124] Setting up drop4
I1204 14:36:38.515401 17293 net.cpp:131] Top shape: 64 500 (32000)
I1204 14:36:38.515462 17293 net.cpp:139] Memory required for data: 182896128
I1204 14:36:38.515475 17293 layer_factory.hpp:77] Creating layer fc6
I1204 14:36:38.515496 17293 net.cpp:86] Creating Layer fc6
I1204 14:36:38.515506 17293 net.cpp:408] fc6 <- fc5
I1204 14:36:38.515524 17293 net.cpp:382] fc6 -> fc6
I1204 14:36:38.516010 17293 net.cpp:124] Setting up fc6
I1204 14:36:38.516031 17293 net.cpp:131] Top shape: 64 30 (1920)
I1204 14:36:38.516047 17293 net.cpp:139] Memory required for data: 182903808
I1204 14:36:38.516082 17293 layer_factory.hpp:77] Creating layer loss
I1204 14:36:38.516100 17293 net.cpp:86] Creating Layer loss
I1204 14:36:38.516110 17293 net.cpp:408] loss <- fc6
I1204 14:36:38.516122 17293 net.cpp:408] loss <- label
I1204 14:36:38.516144 17293 net.cpp:382] loss -> loss
I1204 14:36:38.516237 17293 net.cpp:124] Setting up loss
I1204 14:36:38.516252 17293 net.cpp:131] Top shape: (1)
I1204 14:36:38.516264 17293 net.cpp:134]     with loss weight 1
I1204 14:36:38.516326 17293 net.cpp:139] Memory required for data: 182903812
I1204 14:36:38.516438 17293 net.cpp:200] loss needs backward computation.
I1204 14:36:38.516461 17293 net.cpp:200] fc6 needs backward computation.
I1204 14:36:38.516500 17293 net.cpp:200] drop4 needs backward computation.
I1204 14:36:38.516511 17293 net.cpp:200] fc5 needs backward computation.
I1204 14:36:38.516521 17293 net.cpp:200] dropout3 needs backward computation.
I1204 14:36:38.516533 17293 net.cpp:200] relu3 needs backward computation.
I1204 14:36:38.516542 17293 net.cpp:200] conv3 needs backward computation.
I1204 14:36:38.516553 17293 net.cpp:200] dropout2 needs backward computation.
I1204 14:36:38.516564 17293 net.cpp:200] pool2 needs backward computation.
I1204 14:36:38.516575 17293 net.cpp:200] relu2 needs backward computation.
I1204 14:36:38.516584 17293 net.cpp:200] conv2 needs backward computation.
I1204 14:36:38.516597 17293 net.cpp:200] dropout1 needs backward computation.
I1204 14:36:38.516607 17293 net.cpp:200] pool1 needs backward computation.
I1204 14:36:38.516618 17293 net.cpp:200] relu1 needs backward computation.
I1204 14:36:38.516628 17293 net.cpp:200] conv1 needs backward computation.
I1204 14:36:38.516639 17293 net.cpp:202] MyData does not need backward computation.
I1204 14:36:38.516649 17293 net.cpp:244] This network produces output loss
I1204 14:36:38.516686 17293 net.cpp:257] Network initialization done.
I1204 14:36:38.517518 17293 solver.cpp:190] Creating test net (#0) specified by net file: models/Example/example.prototxt
I1204 14:36:38.517612 17293 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer MyData
I1204 14:36:38.517679 17293 net.cpp:53] Initializing net from parameters: 
name: "example_Network"
state {
  phase: TEST
}
layer {
  name: "MyData"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "dataset/test.txt"
    batch_size: 100
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
      variance_norm: AVERAGE
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "dropout1"
  type: "Dropout"
  bottom: "pool1"
  top: "pool1"
  dropout_param {
    dropout_ratio: 0.1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 48
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
      variance_norm: AVERAGE
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "dropout2"
  type: "Dropout"
  bottom: "pool2"
  top: "pool2"
  dropout_param {
    dropout_ratio: 0.3
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      variance_norm: AVERAGE
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "dropout3"
  type: "Dropout"
  bottom: "conv3"
  top: "conv3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc5"
  type: "InnerProduct"
  bottom: "conv3"
  top: "fc5"
  param {
    lr_mult: 10
    decay_mult: 1
  }
  param {
    lr_mult: 20
    decay_mult: 0
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
      variance_norm: AVERAGE
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "drop4"
  type: "Dropout"
  bottom: "fc5"
  top: "fc5"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "fc5"
  top: "fc6"
  param {
    lr_mult: 10
    decay_mult: 1
  }
  param {
    lr_mult: 20
    decay_mult: 0
  }
  inner_product_param {
    num_output: 30
    weight_filler {
      type: "xavier"
      variance_norm: AVERAGE
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "EuclideanLoss"
  bottom: "fc6"
  bottom: "label"
  top: "loss"
}
I1204 14:36:38.518355 17293 layer_factory.hpp:77] Creating layer MyData
I1204 14:36:38.518385 17293 net.cpp:86] Creating Layer MyData
I1204 14:36:38.518401 17293 net.cpp:382] MyData -> data
I1204 14:36:38.518424 17293 net.cpp:382] MyData -> label
I1204 14:36:38.518443 17293 hdf5_data_layer.cpp:81] Loading list of HDF5 filenames from: dataset/test.txt
I1204 14:36:38.518508 17293 hdf5_data_layer.cpp:95] Number of HDF5 files: 1
I1204 14:36:38.747522 17293 net.cpp:124] Setting up MyData
I1204 14:36:38.747589 17293 net.cpp:131] Top shape: 100 1 96 96 (921600)
I1204 14:36:38.747620 17293 net.cpp:131] Top shape: 100 30 (3000)
I1204 14:36:38.747637 17293 net.cpp:139] Memory required for data: 3698400
I1204 14:36:38.747658 17293 layer_factory.hpp:77] Creating layer conv1
I1204 14:36:38.747704 17293 net.cpp:86] Creating Layer conv1
I1204 14:36:38.747715 17293 net.cpp:408] conv1 <- data
I1204 14:36:38.747738 17293 net.cpp:382] conv1 -> conv1
I1204 14:36:38.748466 17293 net.cpp:124] Setting up conv1
I1204 14:36:38.748489 17293 net.cpp:131] Top shape: 100 20 92 92 (16928000)
I1204 14:36:38.748507 17293 net.cpp:139] Memory required for data: 71410400
I1204 14:36:38.748536 17293 layer_factory.hpp:77] Creating layer relu1
I1204 14:36:38.748558 17293 net.cpp:86] Creating Layer relu1
I1204 14:36:38.748569 17293 net.cpp:408] relu1 <- conv1
I1204 14:36:38.748584 17293 net.cpp:369] relu1 -> conv1 (in-place)
I1204 14:36:38.748602 17293 net.cpp:124] Setting up relu1
I1204 14:36:38.748611 17293 net.cpp:131] Top shape: 100 20 92 92 (16928000)
I1204 14:36:38.748626 17293 net.cpp:139] Memory required for data: 139122400
I1204 14:36:38.748636 17293 layer_factory.hpp:77] Creating layer pool1
I1204 14:36:38.748652 17293 net.cpp:86] Creating Layer pool1
I1204 14:36:38.748663 17293 net.cpp:408] pool1 <- conv1
I1204 14:36:38.748677 17293 net.cpp:382] pool1 -> pool1
I1204 14:36:38.748761 17293 net.cpp:124] Setting up pool1
I1204 14:36:38.748775 17293 net.cpp:131] Top shape: 100 20 46 46 (4232000)
I1204 14:36:38.748790 17293 net.cpp:139] Memory required for data: 156050400
I1204 14:36:38.748801 17293 layer_factory.hpp:77] Creating layer dropout1
I1204 14:36:38.748821 17293 net.cpp:86] Creating Layer dropout1
I1204 14:36:38.748831 17293 net.cpp:408] dropout1 <- pool1
I1204 14:36:38.748847 17293 net.cpp:369] dropout1 -> pool1 (in-place)
I1204 14:36:38.748896 17293 net.cpp:124] Setting up dropout1
I1204 14:36:38.748911 17293 net.cpp:131] Top shape: 100 20 46 46 (4232000)
I1204 14:36:38.748981 17293 net.cpp:139] Memory required for data: 172978400
I1204 14:36:38.749001 17293 layer_factory.hpp:77] Creating layer conv2
I1204 14:36:38.749053 17293 net.cpp:86] Creating Layer conv2
I1204 14:36:38.749063 17293 net.cpp:408] conv2 <- pool1
I1204 14:36:38.749081 17293 net.cpp:382] conv2 -> conv2
I1204 14:36:38.750010 17293 net.cpp:124] Setting up conv2
I1204 14:36:38.750036 17293 net.cpp:131] Top shape: 100 48 42 42 (8467200)
I1204 14:36:38.750082 17293 net.cpp:139] Memory required for data: 206847200
I1204 14:36:38.750106 17293 layer_factory.hpp:77] Creating layer relu2
I1204 14:36:38.750197 17293 net.cpp:86] Creating Layer relu2
I1204 14:36:38.750208 17293 net.cpp:408] relu2 <- conv2
I1204 14:36:38.750223 17293 net.cpp:369] relu2 -> conv2 (in-place)
I1204 14:36:38.750306 17293 net.cpp:124] Setting up relu2
I1204 14:36:38.750317 17293 net.cpp:131] Top shape: 100 48 42 42 (8467200)
I1204 14:36:38.750332 17293 net.cpp:139] Memory required for data: 240716000
I1204 14:36:38.750346 17293 layer_factory.hpp:77] Creating layer pool2
I1204 14:36:38.750362 17293 net.cpp:86] Creating Layer pool2
I1204 14:36:38.750372 17293 net.cpp:408] pool2 <- conv2
I1204 14:36:38.750387 17293 net.cpp:382] pool2 -> pool2
I1204 14:36:38.750490 17293 net.cpp:124] Setting up pool2
I1204 14:36:38.750510 17293 net.cpp:131] Top shape: 100 48 21 21 (2116800)
I1204 14:36:38.750526 17293 net.cpp:139] Memory required for data: 249183200
I1204 14:36:38.750537 17293 layer_factory.hpp:77] Creating layer dropout2
I1204 14:36:38.750558 17293 net.cpp:86] Creating Layer dropout2
I1204 14:36:38.750569 17293 net.cpp:408] dropout2 <- pool2
I1204 14:36:38.750583 17293 net.cpp:369] dropout2 -> pool2 (in-place)
I1204 14:36:38.750632 17293 net.cpp:124] Setting up dropout2
I1204 14:36:38.750644 17293 net.cpp:131] Top shape: 100 48 21 21 (2116800)
I1204 14:36:38.750659 17293 net.cpp:139] Memory required for data: 257650400
I1204 14:36:38.750670 17293 layer_factory.hpp:77] Creating layer conv3
I1204 14:36:38.750694 17293 net.cpp:86] Creating Layer conv3
I1204 14:36:38.750733 17293 net.cpp:408] conv3 <- pool2
I1204 14:36:38.750752 17293 net.cpp:382] conv3 -> conv3
I1204 14:36:38.751772 17293 net.cpp:124] Setting up conv3
I1204 14:36:38.751797 17293 net.cpp:131] Top shape: 100 64 19 19 (2310400)
I1204 14:36:38.751816 17293 net.cpp:139] Memory required for data: 266892000
I1204 14:36:38.751842 17293 layer_factory.hpp:77] Creating layer relu3
I1204 14:36:38.751860 17293 net.cpp:86] Creating Layer relu3
I1204 14:36:38.751870 17293 net.cpp:408] relu3 <- conv3
I1204 14:36:38.751885 17293 net.cpp:369] relu3 -> conv3 (in-place)
I1204 14:36:38.751904 17293 net.cpp:124] Setting up relu3
I1204 14:36:38.751912 17293 net.cpp:131] Top shape: 100 64 19 19 (2310400)
I1204 14:36:38.751927 17293 net.cpp:139] Memory required for data: 276133600
I1204 14:36:38.751937 17293 layer_factory.hpp:77] Creating layer dropout3
I1204 14:36:38.751957 17293 net.cpp:86] Creating Layer dropout3
I1204 14:36:38.751969 17293 net.cpp:408] dropout3 <- conv3
I1204 14:36:38.751982 17293 net.cpp:369] dropout3 -> conv3 (in-place)
I1204 14:36:38.752035 17293 net.cpp:124] Setting up dropout3
I1204 14:36:38.752048 17293 net.cpp:131] Top shape: 100 64 19 19 (2310400)
I1204 14:36:38.752063 17293 net.cpp:139] Memory required for data: 285375200
I1204 14:36:38.752071 17293 layer_factory.hpp:77] Creating layer fc5
I1204 14:36:38.752091 17293 net.cpp:86] Creating Layer fc5
I1204 14:36:38.752110 17293 net.cpp:408] fc5 <- conv3
I1204 14:36:38.752128 17293 net.cpp:382] fc5 -> fc5
I1204 14:36:38.936254 17293 net.cpp:124] Setting up fc5
I1204 14:36:38.936309 17293 net.cpp:131] Top shape: 100 500 (50000)
I1204 14:36:38.936338 17293 net.cpp:139] Memory required for data: 285575200
I1204 14:36:38.936369 17293 layer_factory.hpp:77] Creating layer drop4
I1204 14:36:38.936396 17293 net.cpp:86] Creating Layer drop4
I1204 14:36:38.936410 17293 net.cpp:408] drop4 <- fc5
I1204 14:36:38.936429 17293 net.cpp:369] drop4 -> fc5 (in-place)
I1204 14:36:38.936496 17293 net.cpp:124] Setting up drop4
I1204 14:36:38.936508 17293 net.cpp:131] Top shape: 100 500 (50000)
I1204 14:36:38.936550 17293 net.cpp:139] Memory required for data: 285775200
I1204 14:36:38.936561 17293 layer_factory.hpp:77] Creating layer fc6
I1204 14:36:38.936583 17293 net.cpp:86] Creating Layer fc6
I1204 14:36:38.936595 17293 net.cpp:408] fc6 <- fc5
I1204 14:36:38.936614 17293 net.cpp:382] fc6 -> fc6
I1204 14:36:38.937103 17293 net.cpp:124] Setting up fc6
I1204 14:36:38.937126 17293 net.cpp:131] Top shape: 100 30 (3000)
I1204 14:36:38.937141 17293 net.cpp:139] Memory required for data: 285787200
I1204 14:36:38.937172 17293 layer_factory.hpp:77] Creating layer loss
I1204 14:36:38.937191 17293 net.cpp:86] Creating Layer loss
I1204 14:36:38.937201 17293 net.cpp:408] loss <- fc6
I1204 14:36:38.937213 17293 net.cpp:408] loss <- label
I1204 14:36:38.937288 17293 net.cpp:382] loss -> loss
I1204 14:36:38.937386 17293 net.cpp:124] Setting up loss
I1204 14:36:38.937422 17293 net.cpp:131] Top shape: (1)
I1204 14:36:38.937436 17293 net.cpp:134]     with loss weight 1
I1204 14:36:38.937463 17293 net.cpp:139] Memory required for data: 285787204
I1204 14:36:38.937475 17293 net.cpp:200] loss needs backward computation.
I1204 14:36:38.937487 17293 net.cpp:200] fc6 needs backward computation.
I1204 14:36:38.937499 17293 net.cpp:200] drop4 needs backward computation.
I1204 14:36:38.937513 17293 net.cpp:200] fc5 needs backward computation.
I1204 14:36:38.937525 17293 net.cpp:200] dropout3 needs backward computation.
I1204 14:36:38.937536 17293 net.cpp:200] relu3 needs backward computation.
I1204 14:36:38.937561 17293 net.cpp:200] conv3 needs backward computation.
I1204 14:36:38.937572 17293 net.cpp:200] dropout2 needs backward computation.
I1204 14:36:38.937587 17293 net.cpp:200] pool2 needs backward computation.
I1204 14:36:38.937598 17293 net.cpp:200] relu2 needs backward computation.
I1204 14:36:38.937608 17293 net.cpp:200] conv2 needs backward computation.
I1204 14:36:38.937618 17293 net.cpp:200] dropout1 needs backward computation.
I1204 14:36:38.937629 17293 net.cpp:200] pool1 needs backward computation.
I1204 14:36:38.937639 17293 net.cpp:200] relu1 needs backward computation.
I1204 14:36:38.937649 17293 net.cpp:200] conv1 needs backward computation.
I1204 14:36:38.937661 17293 net.cpp:202] MyData does not need backward computation.
I1204 14:36:38.937670 17293 net.cpp:244] This network produces output loss
I1204 14:36:38.937705 17293 net.cpp:257] Network initialization done.
I1204 14:36:38.937914 17293 solver.cpp:57] Solver scaffolding done.
I1204 14:36:38.938863 17293 caffe.cpp:239] Starting Optimization
I1204 14:36:38.938891 17293 solver.cpp:289] Solving example_Network
I1204 14:36:38.938902 17293 solver.cpp:290] Learning Rate Policy: fixed
I1204 14:36:38.950155 17293 solver.cpp:347] Iteration 0, Testing net (#0)
I1204 14:36:39.305789 17293 solver.cpp:414]     Test net output #0: loss = 4.28887 (* 1 = 4.28887 loss)
I1204 14:36:39.594391 17293 solver.cpp:239] Iteration 0 (0 iter/s, 0.647704s/100 iters), loss = 5.86991
I1204 14:36:39.594558 17293 solver.cpp:258]     Train net output #0: loss = 5.86991 (* 1 = 5.86991 loss)
I1204 14:36:39.594590 17293 sgd_solver.cpp:112] Iteration 0, lr = 0.001
I1204 14:37:05.955265 17293 solver.cpp:239] Iteration 100 (3.79353 iter/s, 26.3607s/100 iters), loss = 0.0195095
I1204 14:37:05.955441 17293 solver.cpp:258]     Train net output #0: loss = 0.01951 (* 1 = 0.01951 loss)
I1204 14:37:05.955469 17293 sgd_solver.cpp:112] Iteration 100, lr = 0.001
I1204 14:37:32.295765 17293 solver.cpp:239] Iteration 200 (3.79646 iter/s, 26.3403s/100 iters), loss = 0.0148505
I1204 14:37:32.296061 17293 solver.cpp:258]     Train net output #0: loss = 0.014851 (* 1 = 0.014851 loss)
I1204 14:37:32.296093 17293 sgd_solver.cpp:112] Iteration 200, lr = 0.001
I1204 14:37:58.666087 17293 solver.cpp:239] Iteration 300 (3.79219 iter/s, 26.37s/100 iters), loss = 0.0135988
I1204 14:37:58.666270 17293 solver.cpp:258]     Train net output #0: loss = 0.0135993 (* 1 = 0.0135993 loss)
I1204 14:37:58.666302 17293 sgd_solver.cpp:112] Iteration 300, lr = 0.001
I1204 14:38:24.988406 17293 solver.cpp:239] Iteration 400 (3.79908 iter/s, 26.3221s/100 iters), loss = 0.0129764
I1204 14:38:24.988756 17293 solver.cpp:258]     Train net output #0: loss = 0.0129769 (* 1 = 0.0129769 loss)
I1204 14:38:24.988790 17293 sgd_solver.cpp:112] Iteration 400, lr = 0.001
I1204 14:38:50.954988 17293 solver.cpp:347] Iteration 500, Testing net (#0)
I1204 14:38:51.216599 17293 solver.cpp:414]     Test net output #0: loss = 0.0259013 (* 1 = 0.0259013 loss)
I1204 14:38:51.465719 17293 solver.cpp:239] Iteration 500 (3.77688 iter/s, 26.4769s/100 iters), loss = 0.0123011
I1204 14:38:51.465888 17293 solver.cpp:258]     Train net output #0: loss = 0.0123016 (* 1 = 0.0123016 loss)
I1204 14:38:51.465919 17293 sgd_solver.cpp:112] Iteration 500, lr = 0.001
I1204 14:39:17.841570 17293 solver.cpp:239] Iteration 600 (3.79138 iter/s, 26.3757s/100 iters), loss = 0.0126668
I1204 14:39:17.841926 17293 solver.cpp:258]     Train net output #0: loss = 0.0126673 (* 1 = 0.0126673 loss)
I1204 14:39:17.841985 17293 sgd_solver.cpp:112] Iteration 600, lr = 0.001
I1204 14:39:44.178853 17293 solver.cpp:239] Iteration 700 (3.79696 iter/s, 26.3369s/100 iters), loss = 0.0119203
I1204 14:39:44.179013 17293 solver.cpp:258]     Train net output #0: loss = 0.0119208 (* 1 = 0.0119208 loss)
I1204 14:39:44.179059 17293 sgd_solver.cpp:112] Iteration 700, lr = 0.001
I1204 14:40:10.550942 17293 solver.cpp:239] Iteration 800 (3.79192 iter/s, 26.3719s/100 iters), loss = 0.0142254
I1204 14:40:10.551224 17293 solver.cpp:258]     Train net output #0: loss = 0.0142259 (* 1 = 0.0142259 loss)
I1204 14:40:10.551254 17293 sgd_solver.cpp:112] Iteration 800, lr = 0.001
I1204 14:40:36.888520 17293 solver.cpp:239] Iteration 900 (3.79691 iter/s, 26.3372s/100 iters), loss = 0.011535
I1204 14:40:36.888682 17293 solver.cpp:258]     Train net output #0: loss = 0.0115355 (* 1 = 0.0115355 loss)
I1204 14:40:36.888731 17293 sgd_solver.cpp:112] Iteration 900, lr = 0.001
I1204 14:41:02.852843 17293 solver.cpp:464] Snapshotting to binary proto file /home/nvidia/.local/install/caffe/models/Example/solver_iter_1000.caffemodel
I1204 14:41:04.272162 17293 sgd_solver.cpp:284] Snapshotting solver state to binary proto file /home/nvidia/.local/install/caffe/models/Example/solver_iter_1000.solverstate
I1204 14:41:04.415760 17293 solver.cpp:347] Iteration 1000, Testing net (#0)
I1204 14:41:04.614655 17293 solver.cpp:414]     Test net output #0: loss = 0.0329075 (* 1 = 0.0329075 loss)
I1204 14:41:04.880563 17293 solver.cpp:239] Iteration 1000 (3.57247 iter/s, 27.9919s/100 iters), loss = 0.0108015
I1204 14:41:04.880759 17293 solver.cpp:258]     Train net output #0: loss = 0.010802 (* 1 = 0.010802 loss)
I1204 14:41:04.880820 17293 sgd_solver.cpp:112] Iteration 1000, lr = 0.001
I1204 14:41:31.196699 17293 solver.cpp:239] Iteration 1100 (3.79999 iter/s, 26.3159s/100 iters), loss = 0.0135497
I1204 14:41:31.196861 17293 solver.cpp:258]     Train net output #0: loss = 0.0135502 (* 1 = 0.0135502 loss)
I1204 14:41:31.196902 17293 sgd_solver.cpp:112] Iteration 1100, lr = 0.001
I1204 14:41:57.523921 17293 solver.cpp:239] Iteration 1200 (3.79838 iter/s, 26.327s/100 iters), loss = 0.0109335
I1204 14:41:57.524186 17293 solver.cpp:258]     Train net output #0: loss = 0.010934 (* 1 = 0.010934 loss)
I1204 14:41:57.524214 17293 sgd_solver.cpp:112] Iteration 1200, lr = 0.001
I1204 14:42:23.829288 17293 solver.cpp:239] Iteration 1300 (3.80157 iter/s, 26.305s/100 iters), loss = 0.014287
I1204 14:42:23.829435 17293 solver.cpp:258]     Train net output #0: loss = 0.0142875 (* 1 = 0.0142875 loss)
I1204 14:42:23.829494 17293 sgd_solver.cpp:112] Iteration 1300, lr = 0.001
I1204 14:42:50.196671 17293 solver.cpp:239] Iteration 1400 (3.79259 iter/s, 26.3672s/100 iters), loss = 0.0133881
I1204 14:42:50.197022 17293 solver.cpp:258]     Train net output #0: loss = 0.0133886 (* 1 = 0.0133886 loss)
I1204 14:42:50.197055 17293 sgd_solver.cpp:112] Iteration 1400, lr = 0.001
I1204 14:43:16.142093 17293 solver.cpp:347] Iteration 1500, Testing net (#0)
I1204 14:43:16.416723 17293 solver.cpp:414]     Test net output #0: loss = 0.0295172 (* 1 = 0.0295172 loss)
I1204 14:43:16.667857 17293 solver.cpp:239] Iteration 1500 (3.77775 iter/s, 26.4708s/100 iters), loss = 0.0130599
I1204 14:43:16.668025 17293 solver.cpp:258]     Train net output #0: loss = 0.0130604 (* 1 = 0.0130604 loss)
I1204 14:43:16.668057 17293 sgd_solver.cpp:112] Iteration 1500, lr = 0.001
I1204 14:43:43.014950 17293 solver.cpp:239] Iteration 1600 (3.79551 iter/s, 26.3469s/100 iters), loss = 0.0124526
I1204 14:43:43.015383 17293 solver.cpp:258]     Train net output #0: loss = 0.012453 (* 1 = 0.012453 loss)
I1204 14:43:43.015414 17293 sgd_solver.cpp:112] Iteration 1600, lr = 0.001
I1204 14:44:09.354365 17293 solver.cpp:239] Iteration 1700 (3.79666 iter/s, 26.339s/100 iters), loss = 0.0137831
I1204 14:44:09.354498 17293 solver.cpp:258]     Train net output #0: loss = 0.0137835 (* 1 = 0.0137835 loss)
I1204 14:44:09.354529 17293 sgd_solver.cpp:112] Iteration 1700, lr = 0.001
I1204 14:44:35.674890 17293 solver.cpp:239] Iteration 1800 (3.79934 iter/s, 26.3203s/100 iters), loss = 0.0117692
I1204 14:44:35.675175 17293 solver.cpp:258]     Train net output #0: loss = 0.0117697 (* 1 = 0.0117697 loss)
I1204 14:44:35.675209 17293 sgd_solver.cpp:112] Iteration 1800, lr = 0.001
I1204 14:45:02.033674 17293 solver.cpp:239] Iteration 1900 (3.79385 iter/s, 26.3585s/100 iters), loss = 0.0138034
I1204 14:45:02.033849 17293 solver.cpp:258]     Train net output #0: loss = 0.0138039 (* 1 = 0.0138039 loss)
I1204 14:45:02.033885 17293 sgd_solver.cpp:112] Iteration 1900, lr = 0.001
I1204 14:45:28.030046 17293 solver.cpp:464] Snapshotting to binary proto file /home/nvidia/.local/install/caffe/models/Example/solver_iter_2000.caffemodel
I1204 14:45:29.385005 17293 sgd_solver.cpp:284] Snapshotting solver state to binary proto file /home/nvidia/.local/install/caffe/models/Example/solver_iter_2000.solverstate
I1204 14:45:29.530635 17293 solver.cpp:347] Iteration 2000, Testing net (#0)
I1204 14:45:29.707396 17293 solver.cpp:414]     Test net output #0: loss = 0.0295899 (* 1 = 0.0295899 loss)
I1204 14:45:29.974252 17293 solver.cpp:239] Iteration 2000 (3.57904 iter/s, 27.9404s/100 iters), loss = 0.0107167
I1204 14:45:29.974422 17293 solver.cpp:258]     Train net output #0: loss = 0.0107172 (* 1 = 0.0107172 loss)
I1204 14:45:29.974453 17293 sgd_solver.cpp:112] Iteration 2000, lr = 0.001
I1204 14:45:56.309207 17293 solver.cpp:239] Iteration 2100 (3.79726 iter/s, 26.3348s/100 iters), loss = 0.0102244
I1204 14:45:56.309345 17293 solver.cpp:258]     Train net output #0: loss = 0.0102249 (* 1 = 0.0102249 loss)
I1204 14:45:56.309376 17293 sgd_solver.cpp:112] Iteration 2100, lr = 0.001
I1204 14:46:22.667318 17293 solver.cpp:239] Iteration 2200 (3.79392 iter/s, 26.3579s/100 iters), loss = 0.0104493
I1204 14:46:22.667649 17293 solver.cpp:258]     Train net output #0: loss = 0.0104498 (* 1 = 0.0104498 loss)
I1204 14:46:22.667680 17293 sgd_solver.cpp:112] Iteration 2200, lr = 0.001
I1204 14:46:49.026518 17293 solver.cpp:239] Iteration 2300 (3.79379 iter/s, 26.3588s/100 iters), loss = 0.0118667
I1204 14:46:49.026661 17293 solver.cpp:258]     Train net output #0: loss = 0.0118672 (* 1 = 0.0118672 loss)
I1204 14:46:49.026690 17293 sgd_solver.cpp:112] Iteration 2300, lr = 0.001
I1204 14:47:15.361508 17293 solver.cpp:239] Iteration 2400 (3.79728 iter/s, 26.3346s/100 iters), loss = 0.0107545
I1204 14:47:15.361858 17293 solver.cpp:258]     Train net output #0: loss = 0.010755 (* 1 = 0.010755 loss)
I1204 14:47:15.361891 17293 sgd_solver.cpp:112] Iteration 2400, lr = 0.001
I1204 14:47:41.329602 17293 solver.cpp:347] Iteration 2500, Testing net (#0)
I1204 14:47:41.591483 17293 solver.cpp:414]     Test net output #0: loss = 0.0271688 (* 1 = 0.0271688 loss)
I1204 14:47:41.841346 17293 solver.cpp:239] Iteration 2500 (3.7765 iter/s, 26.4795s/100 iters), loss = 0.0129609
I1204 14:47:41.841504 17293 solver.cpp:258]     Train net output #0: loss = 0.0129613 (* 1 = 0.0129613 loss)
I1204 14:47:41.841536 17293 sgd_solver.cpp:112] Iteration 2500, lr = 0.001
I1204 14:48:08.191920 17293 solver.cpp:239] Iteration 2600 (3.79501 iter/s, 26.3504s/100 iters), loss = 0.0102999
I1204 14:48:08.192209 17293 solver.cpp:258]     Train net output #0: loss = 0.0103004 (* 1 = 0.0103004 loss)
I1204 14:48:08.192270 17293 sgd_solver.cpp:112] Iteration 2600, lr = 0.001
I1204 14:48:34.559082 17293 solver.cpp:239] Iteration 2700 (3.79264 iter/s, 26.3669s/100 iters), loss = 0.00953375
I1204 14:48:34.559247 17293 solver.cpp:258]     Train net output #0: loss = 0.00953422 (* 1 = 0.00953422 loss)
I1204 14:48:34.559278 17293 sgd_solver.cpp:112] Iteration 2700, lr = 0.001
I1204 14:49:00.891937 17293 solver.cpp:239] Iteration 2800 (3.79756 iter/s, 26.3327s/100 iters), loss = 0.0113248
I1204 14:49:00.892263 17293 solver.cpp:258]     Train net output #0: loss = 0.0113253 (* 1 = 0.0113253 loss)
I1204 14:49:00.892295 17293 sgd_solver.cpp:112] Iteration 2800, lr = 0.001
I1204 14:49:27.254297 17293 solver.cpp:239] Iteration 2900 (3.79334 iter/s, 26.362s/100 iters), loss = 0.00965072
I1204 14:49:27.254482 17293 solver.cpp:258]     Train net output #0: loss = 0.0096512 (* 1 = 0.0096512 loss)
I1204 14:49:27.254519 17293 sgd_solver.cpp:112] Iteration 2900, lr = 0.001
I1204 14:49:53.223443 17293 solver.cpp:464] Snapshotting to binary proto file /home/nvidia/.local/install/caffe/models/Example/solver_iter_3000.caffemodel
I1204 14:49:54.142555 17293 sgd_solver.cpp:284] Snapshotting solver state to binary proto file /home/nvidia/.local/install/caffe/models/Example/solver_iter_3000.solverstate
I1204 14:49:54.580802 17293 solver.cpp:327] Iteration 3000, loss = 0.0110822
I1204 14:49:54.580920 17293 solver.cpp:347] Iteration 3000, Testing net (#0)
I1204 14:49:54.777360 17293 solver.cpp:414]     Test net output #0: loss = 0.0289628 (* 1 = 0.0289628 loss)
I1204 14:49:54.777443 17293 solver.cpp:332] Optimization Done.
I1204 14:49:54.777468 17293 caffe.cpp:250] Optimization Done.
